{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import gym_ple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flappy Bird Sem Visão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlappyBirdEnv(gym.Env):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.env = gym.make(\"FlappyBird-v0\")\n",
    "    # Define action and observation space\n",
    "    # They must be gym.spaces objects\n",
    "    # Example when using discrete actions:\n",
    "    self.action_space = self.env.action_space\n",
    "    # Example for using image as input:\n",
    "    self.observation_space = spaces.Box(low=np.array([0, -10.0, 0, 0, 0, 0, 0, 0]),\n",
    "                                        high=np.array([512, 10.0, 588.0, 512, 512, 588.0, 512, 512]),\n",
    "                                        dtype=np.float32)\n",
    "\n",
    "  def step(self, action):\n",
    "    observation, reward, done, info = self.env.step(action)\n",
    "    observation = np.array(list(self.env.game_state.getGameState().values()))\n",
    "    if done:\n",
    "        reward = -1\n",
    "    reward += 0.1\n",
    "#     reward += (75 - abs(observation[0] - (observation[3] + observation[4])/2))*max((300 - observation[2])/300, 0)/750\n",
    "    return observation, reward, done, info\n",
    "\n",
    "  def reset(self):\n",
    "    self.env.reset()\n",
    "    observation = np.array(list(self.env.game_state.getGameState().values()))\n",
    "    return observation  # reward, done, info can't be included\n",
    "\n",
    "  def render(self):\n",
    "    self.env.render()\n",
    "    \n",
    "  def close (self):\n",
    "    self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.env_checker import check_env\n",
    "\n",
    "env = FlappyBirdEnv()\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common import set_global_seeds, make_vec_env\n",
    "# from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "env = make_vec_env(FlappyBirdEnv, n_envs = 16)\n",
    "# Optional: PPO2 requires a vectorized environment to run\n",
    "# the env is now wrapped automatically when passing it to the constructor\n",
    "# env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F966212438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F966212438>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F966212438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F966212438>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F966226C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F966226C18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F966226C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F966226C18>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "model = PPO2(MlpPolicy, env, n_steps = 512, nminibatches = 64, lam = 0.98, gamma = 0.99, noptepochs= 10, ent_coef= 0.001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F92E3578D0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F92E3578D0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F92E3578D0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F92E3578D0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F972CD2B38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F972CD2B38>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F972CD2B38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F972CD2B38>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "model = PPO2.load(\"trained_models/PPO2\")\n",
    "model.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| approxkl           | 0.018743424  |\n",
      "| clipfrac           | 0.08632813   |\n",
      "| ep_len_mean        | 618          |\n",
      "| ep_reward_mean     | 107          |\n",
      "| explained_variance | 0.59         |\n",
      "| fps                | 338          |\n",
      "| n_updates          | 1            |\n",
      "| policy_entropy     | 0.20740166   |\n",
      "| policy_loss        | 0.0007544595 |\n",
      "| serial_timesteps   | 512          |\n",
      "| time_elapsed       | 0            |\n",
      "| total_timesteps    | 8192         |\n",
      "| value_loss         | 1.624325     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| approxkl           | 0.015732259 |\n",
      "| clipfrac           | 0.08270264  |\n",
      "| ep_len_mean        | 616         |\n",
      "| ep_reward_mean     | 107         |\n",
      "| explained_variance | 0.462       |\n",
      "| fps                | 340         |\n",
      "| n_updates          | 2           |\n",
      "| policy_entropy     | 0.21342364  |\n",
      "| policy_loss        | 0.003289494 |\n",
      "| serial_timesteps   | 1024        |\n",
      "| time_elapsed       | 24.2        |\n",
      "| total_timesteps    | 16384       |\n",
      "| value_loss         | 0.99361503  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.01577675   |\n",
      "| clipfrac           | 0.08372803   |\n",
      "| ep_len_mean        | 628          |\n",
      "| ep_reward_mean     | 109          |\n",
      "| explained_variance | 0.462        |\n",
      "| fps                | 344          |\n",
      "| n_updates          | 3            |\n",
      "| policy_entropy     | 0.20897086   |\n",
      "| policy_loss        | 0.0018805747 |\n",
      "| serial_timesteps   | 1536         |\n",
      "| time_elapsed       | 48.3         |\n",
      "| total_timesteps    | 24576        |\n",
      "| value_loss         | 1.3389853    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.023554873   |\n",
      "| clipfrac           | 0.06881104    |\n",
      "| ep_len_mean        | 717           |\n",
      "| ep_reward_mean     | 126           |\n",
      "| explained_variance | 0.398         |\n",
      "| fps                | 342           |\n",
      "| n_updates          | 4             |\n",
      "| policy_entropy     | 0.19390456    |\n",
      "| policy_loss        | 0.00023412099 |\n",
      "| serial_timesteps   | 2048          |\n",
      "| time_elapsed       | 72            |\n",
      "| total_timesteps    | 32768         |\n",
      "| value_loss         | 2.3318975     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.016184906  |\n",
      "| clipfrac           | 0.06905518   |\n",
      "| ep_len_mean        | 735          |\n",
      "| ep_reward_mean     | 129          |\n",
      "| explained_variance | 0.54         |\n",
      "| fps                | 338          |\n",
      "| n_updates          | 5            |\n",
      "| policy_entropy     | 0.1730999    |\n",
      "| policy_loss        | 0.0025305029 |\n",
      "| serial_timesteps   | 2560         |\n",
      "| time_elapsed       | 95.9         |\n",
      "| total_timesteps    | 40960        |\n",
      "| value_loss         | 0.9005974    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.014485098   |\n",
      "| clipfrac           | 0.06599121    |\n",
      "| ep_len_mean        | 806           |\n",
      "| ep_reward_mean     | 142           |\n",
      "| explained_variance | 0.359         |\n",
      "| fps                | 314           |\n",
      "| n_updates          | 6             |\n",
      "| policy_entropy     | 0.19678137    |\n",
      "| policy_loss        | -0.0007066123 |\n",
      "| serial_timesteps   | 3072          |\n",
      "| time_elapsed       | 120           |\n",
      "| total_timesteps    | 49152         |\n",
      "| value_loss         | 1.9979553     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.013679606  |\n",
      "| clipfrac           | 0.06499024   |\n",
      "| ep_len_mean        | 797          |\n",
      "| ep_reward_mean     | 141          |\n",
      "| explained_variance | 0.525        |\n",
      "| fps                | 314          |\n",
      "| n_updates          | 7            |\n",
      "| policy_entropy     | 0.187727     |\n",
      "| policy_loss        | 0.0016195104 |\n",
      "| serial_timesteps   | 3584         |\n",
      "| time_elapsed       | 146          |\n",
      "| total_timesteps    | 57344        |\n",
      "| value_loss         | 1.6884203    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.00713396   |\n",
      "| clipfrac           | 0.04625244   |\n",
      "| ep_len_mean        | 827          |\n",
      "| ep_reward_mean     | 147          |\n",
      "| explained_variance | 0.537        |\n",
      "| fps                | 322          |\n",
      "| n_updates          | 8            |\n",
      "| policy_entropy     | 0.208334     |\n",
      "| policy_loss        | 0.0005815208 |\n",
      "| serial_timesteps   | 4096         |\n",
      "| time_elapsed       | 172          |\n",
      "| total_timesteps    | 65536        |\n",
      "| value_loss         | 2.9563751    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.017634477  |\n",
      "| clipfrac           | 0.06992187   |\n",
      "| ep_len_mean        | 848          |\n",
      "| ep_reward_mean     | 151          |\n",
      "| explained_variance | 0.586        |\n",
      "| fps                | 321          |\n",
      "| n_updates          | 9            |\n",
      "| policy_entropy     | 0.19601104   |\n",
      "| policy_loss        | 0.0033608186 |\n",
      "| serial_timesteps   | 4608         |\n",
      "| time_elapsed       | 198          |\n",
      "| total_timesteps    | 73728        |\n",
      "| value_loss         | 3.9369984    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| approxkl           | 0.02586382  |\n",
      "| clipfrac           | 0.080114745 |\n",
      "| ep_len_mean        | 876         |\n",
      "| ep_reward_mean     | 156         |\n",
      "| explained_variance | 0.522       |\n",
      "| fps                | 321         |\n",
      "| n_updates          | 10          |\n",
      "| policy_entropy     | 0.20548983  |\n",
      "| policy_loss        | 0.002351382 |\n",
      "| serial_timesteps   | 5120        |\n",
      "| time_elapsed       | 223         |\n",
      "| total_timesteps    | 81920       |\n",
      "| value_loss         | 2.6276422   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| approxkl           | 0.017192911 |\n",
      "| clipfrac           | 0.07197265  |\n",
      "| ep_len_mean        | 914         |\n",
      "| ep_reward_mean     | 163         |\n",
      "| explained_variance | 0.46        |\n",
      "| fps                | 314         |\n",
      "| n_updates          | 11          |\n",
      "| policy_entropy     | 0.19620126  |\n",
      "| policy_loss        | 0.003250753 |\n",
      "| serial_timesteps   | 5632        |\n",
      "| time_elapsed       | 249         |\n",
      "| total_timesteps    | 90112       |\n",
      "| value_loss         | 2.8547816   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| approxkl           | 0.020808414 |\n",
      "| clipfrac           | 0.08105469  |\n",
      "| ep_len_mean        | 932         |\n",
      "| ep_reward_mean     | 167         |\n",
      "| explained_variance | 0.528       |\n",
      "| fps                | 325         |\n",
      "| n_updates          | 12          |\n",
      "| policy_entropy     | 0.20920964  |\n",
      "| policy_loss        | 0.003124723 |\n",
      "| serial_timesteps   | 6144        |\n",
      "| time_elapsed       | 275         |\n",
      "| total_timesteps    | 98304       |\n",
      "| value_loss         | 2.197725    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.019617101  |\n",
      "| clipfrac           | 0.070166014  |\n",
      "| ep_len_mean        | 945          |\n",
      "| ep_reward_mean     | 169          |\n",
      "| explained_variance | 0.416        |\n",
      "| fps                | 319          |\n",
      "| n_updates          | 13           |\n",
      "| policy_entropy     | 0.20702326   |\n",
      "| policy_loss        | 0.0019698308 |\n",
      "| serial_timesteps   | 6656         |\n",
      "| time_elapsed       | 300          |\n",
      "| total_timesteps    | 106496       |\n",
      "| value_loss         | 2.134226     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.022981098  |\n",
      "| clipfrac           | 0.08851318   |\n",
      "| ep_len_mean        | 964          |\n",
      "| ep_reward_mean     | 173          |\n",
      "| explained_variance | 0.619        |\n",
      "| fps                | 318          |\n",
      "| n_updates          | 14           |\n",
      "| policy_entropy     | 0.21222742   |\n",
      "| policy_loss        | 0.0024946686 |\n",
      "| serial_timesteps   | 7168         |\n",
      "| time_elapsed       | 325          |\n",
      "| total_timesteps    | 114688       |\n",
      "| value_loss         | 1.2245916    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| approxkl           | 0.024676885 |\n",
      "| clipfrac           | 0.08376465  |\n",
      "| ep_len_mean        | 921         |\n",
      "| ep_reward_mean     | 165         |\n",
      "| explained_variance | 0.45        |\n",
      "| fps                | 324         |\n",
      "| n_updates          | 15          |\n",
      "| policy_entropy     | 0.23941931  |\n",
      "| policy_loss        | 0.00454756  |\n",
      "| serial_timesteps   | 7680        |\n",
      "| time_elapsed       | 351         |\n",
      "| total_timesteps    | 122880      |\n",
      "| value_loss         | 2.3124585   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| approxkl           | 0.02410226     |\n",
      "| clipfrac           | 0.07977295     |\n",
      "| ep_len_mean        | 905            |\n",
      "| ep_reward_mean     | 162            |\n",
      "| explained_variance | 0.399          |\n",
      "| fps                | 322            |\n",
      "| n_updates          | 16             |\n",
      "| policy_entropy     | 0.24261951     |\n",
      "| policy_loss        | -0.00018298425 |\n",
      "| serial_timesteps   | 8192           |\n",
      "| time_elapsed       | 376            |\n",
      "| total_timesteps    | 131072         |\n",
      "| value_loss         | 2.439866       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.02590335   |\n",
      "| clipfrac           | 0.08952637   |\n",
      "| ep_len_mean        | 900          |\n",
      "| ep_reward_mean     | 161          |\n",
      "| explained_variance | 0.279        |\n",
      "| fps                | 326          |\n",
      "| n_updates          | 17           |\n",
      "| policy_entropy     | 0.26802757   |\n",
      "| policy_loss        | 3.569468e-05 |\n",
      "| serial_timesteps   | 8704         |\n",
      "| time_elapsed       | 402          |\n",
      "| total_timesteps    | 139264       |\n",
      "| value_loss         | 3.5779357    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| approxkl           | 0.049026344 |\n",
      "| clipfrac           | 0.115795895 |\n",
      "| ep_len_mean        | 869         |\n",
      "| ep_reward_mean     | 155         |\n",
      "| explained_variance | 0.288       |\n",
      "| fps                | 315         |\n",
      "| n_updates          | 18          |\n",
      "| policy_entropy     | 0.24763472  |\n",
      "| policy_loss        | 0.003367717 |\n",
      "| serial_timesteps   | 9216        |\n",
      "| time_elapsed       | 427         |\n",
      "| total_timesteps    | 147456      |\n",
      "| value_loss         | 2.5376744   |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| approxkl           | 0.015665004    |\n",
      "| clipfrac           | 0.08452149     |\n",
      "| ep_len_mean        | 836            |\n",
      "| ep_reward_mean     | 149            |\n",
      "| explained_variance | 0.437          |\n",
      "| fps                | 325            |\n",
      "| n_updates          | 19             |\n",
      "| policy_entropy     | 0.24815316     |\n",
      "| policy_loss        | -0.00050310604 |\n",
      "| serial_timesteps   | 9728           |\n",
      "| time_elapsed       | 453            |\n",
      "| total_timesteps    | 155648         |\n",
      "| value_loss         | 3.153203       |\n",
      "---------------------------------------\n",
      "-----------------------------------\n",
      "| approxkl           | 0.04477416 |\n",
      "| clipfrac           | 0.08258057 |\n",
      "| ep_len_mean        | 870        |\n",
      "| ep_reward_mean     | 155        |\n",
      "| explained_variance | 0.492      |\n",
      "| fps                | 326        |\n",
      "| n_updates          | 20         |\n",
      "| policy_entropy     | 0.22740862 |\n",
      "| policy_loss        | 0.0071548  |\n",
      "| serial_timesteps   | 10240      |\n",
      "| time_elapsed       | 478        |\n",
      "| total_timesteps    | 163840     |\n",
      "| value_loss         | 3.0071034  |\n",
      "-----------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.020787645  |\n",
      "| clipfrac           | 0.08499756   |\n",
      "| ep_len_mean        | 834          |\n",
      "| ep_reward_mean     | 149          |\n",
      "| explained_variance | 0.233        |\n",
      "| fps                | 310          |\n",
      "| n_updates          | 21           |\n",
      "| policy_entropy     | 0.23521665   |\n",
      "| policy_loss        | 0.0019235581 |\n",
      "| serial_timesteps   | 10752        |\n",
      "| time_elapsed       | 503          |\n",
      "| total_timesteps    | 172032       |\n",
      "| value_loss         | 1.4653599    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| approxkl           | 0.024268106 |\n",
      "| clipfrac           | 0.08626709  |\n",
      "| ep_len_mean        | 873         |\n",
      "| ep_reward_mean     | 156         |\n",
      "| explained_variance | 0.418       |\n",
      "| fps                | 313         |\n",
      "| n_updates          | 22          |\n",
      "| policy_entropy     | 0.22966573  |\n",
      "| policy_loss        | 0.005616774 |\n",
      "| serial_timesteps   | 11264       |\n",
      "| time_elapsed       | 529         |\n",
      "| total_timesteps    | 180224      |\n",
      "| value_loss         | 3.3898137   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-d4a41f5c24b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    334\u001b[0m                 \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_rollout_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m                 \u001b[1;31m# true_reward is the reward without discount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m                 \u001b[0mrollout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m                 \u001b[1;31m# Unpack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep_infos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\runners.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, callback)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontinue_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\vec_env\\base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \"\"\"\n\u001b[0;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\vec_env\\dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[1;31m# save final observation where user can get it, then reset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stable_baselines\\bench\\monitor.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-79763b3b3c91>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetGameState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\codigos\\rl\\gym-ple\\gym_ple\\ple_env.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_action_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mterminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\codigos\\rl\\gym-ple\\gym_ple\\ple_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mimage_rotated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfliplr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrot90\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetScreenRGB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Hack to fix the rotated image returned by ple\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimage_rotated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\codigos\\rl\\pygame-learning-environment\\ple\\ple.py\u001b[0m in \u001b[0;36mgetScreenRGB\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetScreenRGB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetScreenGrayscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\codigos\\rl\\pygame-learning-environment\\ple\\games\\base\\pygamewrapper.py\u001b[0m in \u001b[0;36mgetScreenRGB\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         return pygame.surfarray.array3d(\n\u001b[1;32m--> 101\u001b[1;33m             pygame.display.get_surface()).astype(np.uint8)\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flappyTest = FlappyBirdEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = flappyTest.reset()\n",
    "dones = 0\n",
    "while not dones:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = flappyTest.step(action)\n",
    "    flappyTest.render()\n",
    "\n",
    "flappyTest.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trained_models/PPO2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flappy Bird CNN LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FlappyBird-v0\"\n",
    "\n",
    "import gym\n",
    "\n",
    "from stable_baselines.common.policies import CnnLstmPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "env_vision = gym.make(ENV_NAME)\n",
    "# Optional: PPO2 requires a vectorized environment to run\n",
    "# the env is now wrapped automatically when passing it to the constructor\n",
    "env_vision = DummyVecEnv([lambda: env_vision])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelVision = PPO2(CnnLstmPolicy, env_vision, n_steps = 512, nminibatches = 1, lam = 0.98, gamma = 0.999, noptepochs= 15, ent_coef= 0.01, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelVision.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env_vision.reset()\n",
    "dones = 0\n",
    "while not dones:\n",
    "    action, _states = modelVision.predict(obs)\n",
    "    obs, rewards, dones, info = env_vision.step(action)\n",
    "    env.render()\n",
    "\n",
    "env_vision.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
