{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Implementação de um Agente em Flappy Bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import gym_ple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flappy Bird Sem Visão Computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiente\n",
    "\n",
    "Modelagem do ambiente do Flappy Bird usando o GameState como observação.\n",
    "\n",
    "#### Observação:\n",
    " * Posição Y do pássaro.\n",
    " * Velocidade Y do pássaro.\n",
    " * Distância do pássaro até o próximo cano.\n",
    " * Posição Y da parte de cima do próximo cano.\n",
    " * Posição Y da parte de baixo do próximo cano.\n",
    " * Distância do pássaro até o cano depois do próximo cano.\n",
    " * Posição Y da parte de cima do cano depois do próximo cano.\n",
    " * Posição Y da parte de baixo do cano depois do próximo cano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlappyBirdEnv(gym.Env):\n",
    "  metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "\n",
    "  def __init__(self):\n",
    "    self.n_pipes = 0\n",
    "    self.env = gym.make(\"FlappyBird-v0\")\n",
    "    # Define action and observation space\n",
    "    # They must be gym.spaces objects\n",
    "    # Example when using discrete actions:\n",
    "    self.action_space = self.env.action_space\n",
    "    # Example for using image as input:\n",
    "    self.observation_space = spaces.Box(low=np.array([0, -10.0, 0, 0, 0, 0, 0, 0]),\n",
    "                                        high=np.array([512, 10.0, 588.0, 512, 512, 588.0, 512, 512]),\n",
    "                                        dtype=np.float32)\n",
    "\n",
    "  def step(self, action):\n",
    "    observation, reward, done, info = self.env.step(action)\n",
    "    observation = np.array(list(self.env.game_state.getGameState().values()))\n",
    "    if reward > 0:\n",
    "        self.n_pipes += 1\n",
    "    if done:\n",
    "        reward = -1\n",
    "    reward += 0.1\n",
    "    reward += (75 - abs(observation[0] - (observation[3] + observation[4])/2))*max((300 - observation[2])/300, 0)/750\n",
    "    \n",
    "    if self.n_pipes >= 500:\n",
    "        done = True\n",
    "    \n",
    "    return observation, reward, done, info\n",
    "\n",
    "  def reset(self):\n",
    "    self.env.reset()\n",
    "    self.n_pipes = 0\n",
    "    observation = np.array(list(self.env.game_state.getGameState().values()))\n",
    "    return observation  # reward, done, info can't be included\n",
    "\n",
    "  def render(self, mode='human'):\n",
    "    self.env.render(mode='human')\n",
    "    \n",
    "  def close (self):\n",
    "    self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checando se o nosso ambiente satisfaz as propriedades do Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'gym_ple.ple_env.PLEEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.env_checker import check_env\n",
    "\n",
    "env = FlappyBirdEnv()\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando um vetor com 16 ambientes para multiprocessamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines.common import set_global_seeds, make_vec_env\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "env = make_vec_env(FlappyBirdEnv, n_envs = 8)\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o modelo de PPO2 com a biblioteca Stable Baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000020F7635E6C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000020F7635E6C8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000020F7635E6C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000020F7635E6C8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000020F26AACF48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000020F26AACF48>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000020F26AACF48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000020F26AACF48>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "model = PPO2(MlpPolicy, env, n_steps = 1024, learning_rate=0.0001, nminibatches = 64, lam = 0.95, gamma = 0.99, noptepochs= 15, ent_coef= 0.0, cliprange= 0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você tem a opção de usar um modelo pré-treinado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000026892A59A48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000026892A59A48>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000026892A59A48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000026892A59A48>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000026892B9F548>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000026892B9F548>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000026892B9F548>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000026892B9F548>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines import PPO2\n",
    "\n",
    "model = PPO2.load(\"trained_models/PPO2_1280\")\n",
    "model.set_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando um Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "\n",
    "EvalEnv = make_vec_env(FlappyBirdEnv, n_envs = 1)\n",
    "EvalEnv = VecNormalize(EvalEnv, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "\n",
    "eval_callback = EvalCallback(EvalEnv, best_model_save_path='./eval_models/',\n",
    "                             log_path='./logs/', n_eval_episodes = 10,\n",
    "                             eval_freq=1024,deterministic=True, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento do Modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=8192, episode_reward=128.81 +/- 75.41\n",
      "Episode length: 723.10 +/- 405.35\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0028525698  |\n",
      "| clipfrac           | 0.022379557   |\n",
      "| ep_len_mean        | 156           |\n",
      "| ep_reward_mean     | 23.1          |\n",
      "| explained_variance | 0.212         |\n",
      "| fps                | 110           |\n",
      "| n_updates          | 1             |\n",
      "| policy_entropy     | 0.4604574     |\n",
      "| policy_loss        | -0.0061988872 |\n",
      "| serial_timesteps   | 1024          |\n",
      "| time_elapsed       | 0             |\n",
      "| total_timesteps    | 8192          |\n",
      "| value_loss         | 1.9070802     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=16384, episode_reward=88.62 +/- 65.62\n",
      "Episode length: 504.30 +/- 347.81\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0026084033 |\n",
      "| clipfrac           | 0.016267903  |\n",
      "| ep_len_mean        | 180          |\n",
      "| ep_reward_mean     | 27.2         |\n",
      "| explained_variance | 0.264        |\n",
      "| fps                | 137          |\n",
      "| n_updates          | 2            |\n",
      "| policy_entropy     | 0.44170612   |\n",
      "| policy_loss        | -0.004194763 |\n",
      "| serial_timesteps   | 2048         |\n",
      "| time_elapsed       | 74           |\n",
      "| total_timesteps    | 16384        |\n",
      "| value_loss         | 2.128388     |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=24576, episode_reward=129.19 +/- 86.65\n",
      "Episode length: 707.70 +/- 452.82\n",
      "New best mean reward!\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0025383672 |\n",
      "| clipfrac           | 0.020930989  |\n",
      "| ep_len_mean        | 214          |\n",
      "| ep_reward_mean     | 33.4         |\n",
      "| explained_variance | 0.256        |\n",
      "| fps                | 116          |\n",
      "| n_updates          | 3            |\n",
      "| policy_entropy     | 0.4183561    |\n",
      "| policy_loss        | -0.00428345  |\n",
      "| serial_timesteps   | 3072         |\n",
      "| time_elapsed       | 134          |\n",
      "| total_timesteps    | 24576        |\n",
      "| value_loss         | 2.1176803    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=32768, episode_reward=62.65 +/- 32.91\n",
      "Episode length: 361.00 +/- 171.43\n",
      "--------------------------------------\n",
      "| approxkl           | 0.003508252   |\n",
      "| clipfrac           | 0.03564453    |\n",
      "| ep_len_mean        | 229           |\n",
      "| ep_reward_mean     | 36            |\n",
      "| explained_variance | 0.184         |\n",
      "| fps                | 153           |\n",
      "| n_updates          | 4             |\n",
      "| policy_entropy     | 0.3957377     |\n",
      "| policy_loss        | -0.0042896695 |\n",
      "| serial_timesteps   | 4096          |\n",
      "| time_elapsed       | 204           |\n",
      "| total_timesteps    | 32768         |\n",
      "| value_loss         | 2.0949872     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=40960, episode_reward=145.15 +/- 135.26\n",
      "Episode length: 794.00 +/- 710.69\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0024952448  |\n",
      "| clipfrac           | 0.023738606   |\n",
      "| ep_len_mean        | 251           |\n",
      "| ep_reward_mean     | 40.1          |\n",
      "| explained_variance | 0.356         |\n",
      "| fps                | 96            |\n",
      "| n_updates          | 5             |\n",
      "| policy_entropy     | 0.40860447    |\n",
      "| policy_loss        | -0.0020985596 |\n",
      "| serial_timesteps   | 5120          |\n",
      "| time_elapsed       | 257           |\n",
      "| total_timesteps    | 40960         |\n",
      "| value_loss         | 2.2167819     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=49152, episode_reward=99.56 +/- 93.14\n",
      "Episode length: 557.20 +/- 492.81\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0028397308  |\n",
      "| clipfrac           | 0.02684733    |\n",
      "| ep_len_mean        | 298           |\n",
      "| ep_reward_mean     | 48.7          |\n",
      "| explained_variance | 0.155         |\n",
      "| fps                | 113           |\n",
      "| n_updates          | 6             |\n",
      "| policy_entropy     | 0.38964596    |\n",
      "| policy_loss        | -0.0036061702 |\n",
      "| serial_timesteps   | 6144          |\n",
      "| time_elapsed       | 342           |\n",
      "| total_timesteps    | 49152         |\n",
      "| value_loss         | 2.3833184     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=57344, episode_reward=104.10 +/- 87.00\n",
      "Episode length: 580.30 +/- 459.39\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0026575092 |\n",
      "| clipfrac           | 0.02947591   |\n",
      "| ep_len_mean        | 319          |\n",
      "| ep_reward_mean     | 52.7         |\n",
      "| explained_variance | 0.316        |\n",
      "| fps                | 116          |\n",
      "| n_updates          | 7            |\n",
      "| policy_entropy     | 0.41202158   |\n",
      "| policy_loss        | -0.004036814 |\n",
      "| serial_timesteps   | 7168         |\n",
      "| time_elapsed       | 414          |\n",
      "| total_timesteps    | 57344        |\n",
      "| value_loss         | 2.162514     |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=65536, episode_reward=154.33 +/- 129.56\n",
      "Episode length: 843.80 +/- 679.77\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0034344944  |\n",
      "| clipfrac           | 0.037654623   |\n",
      "| ep_len_mean        | 352           |\n",
      "| ep_reward_mean     | 58.6          |\n",
      "| explained_variance | 0.21          |\n",
      "| fps                | 95            |\n",
      "| n_updates          | 8             |\n",
      "| policy_entropy     | 0.4044986     |\n",
      "| policy_loss        | -0.0035972765 |\n",
      "| serial_timesteps   | 8192          |\n",
      "| time_elapsed       | 484           |\n",
      "| total_timesteps    | 65536         |\n",
      "| value_loss         | 2.3408837     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=73728, episode_reward=154.09 +/- 90.24\n",
      "Episode length: 851.00 +/- 478.50\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0039904746  |\n",
      "| clipfrac           | 0.04754232    |\n",
      "| ep_len_mean        | 402           |\n",
      "| ep_reward_mean     | 67.8          |\n",
      "| explained_variance | -0.212        |\n",
      "| fps                | 87            |\n",
      "| n_updates          | 9             |\n",
      "| policy_entropy     | 0.40534273    |\n",
      "| policy_loss        | -0.0040321206 |\n",
      "| serial_timesteps   | 9216          |\n",
      "| time_elapsed       | 570           |\n",
      "| total_timesteps    | 73728         |\n",
      "| value_loss         | 2.454631      |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=81920, episode_reward=860.30 +/- 577.63\n",
      "Episode length: 4598.40 +/- 3067.39\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00305465    |\n",
      "| clipfrac           | 0.031526692   |\n",
      "| ep_len_mean        | 450           |\n",
      "| ep_reward_mean     | 76.8          |\n",
      "| explained_variance | -0.548        |\n",
      "| fps                | 24            |\n",
      "| n_updates          | 10            |\n",
      "| policy_entropy     | 0.41108328    |\n",
      "| policy_loss        | -0.0036888232 |\n",
      "| serial_timesteps   | 10240         |\n",
      "| time_elapsed       | 664           |\n",
      "| total_timesteps    | 81920         |\n",
      "| value_loss         | 2.485539      |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=90112, episode_reward=1352.56 +/- 642.88\n",
      "Episode length: 7256.10 +/- 3442.21\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0035704048  |\n",
      "| clipfrac           | 0.03391927    |\n",
      "| ep_len_mean        | 485           |\n",
      "| ep_reward_mean     | 83.3          |\n",
      "| explained_variance | -0.573        |\n",
      "| fps                | 14            |\n",
      "| n_updates          | 11            |\n",
      "| policy_entropy     | 0.37426078    |\n",
      "| policy_loss        | -0.0032999741 |\n",
      "| serial_timesteps   | 11264         |\n",
      "| time_elapsed       | 999           |\n",
      "| total_timesteps    | 90112         |\n",
      "| value_loss         | 2.3963892     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=98304, episode_reward=885.30 +/- 794.87\n",
      "Episode length: 4736.20 +/- 4241.75\n",
      "--------------------------------------\n",
      "| approxkl           | 0.003093857   |\n",
      "| clipfrac           | 0.03461914    |\n",
      "| ep_len_mean        | 598           |\n",
      "| ep_reward_mean     | 104           |\n",
      "| explained_variance | -0.143        |\n",
      "| fps                | 21            |\n",
      "| n_updates          | 12            |\n",
      "| policy_entropy     | 0.40536833    |\n",
      "| policy_loss        | -0.0035851763 |\n",
      "| serial_timesteps   | 12288         |\n",
      "| time_elapsed       | 1.58e+03      |\n",
      "| total_timesteps    | 98304         |\n",
      "| value_loss         | 2.2190628     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=106496, episode_reward=1698.82 +/- 318.99\n",
      "Episode length: 9157.40 +/- 1717.57\n",
      "New best mean reward!\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0019077639 |\n",
      "| clipfrac           | 0.014168294  |\n",
      "| ep_len_mean        | 639          |\n",
      "| ep_reward_mean     | 112          |\n",
      "| explained_variance | -0.418       |\n",
      "| fps                | 11           |\n",
      "| n_updates          | 13           |\n",
      "| policy_entropy     | 0.3629764    |\n",
      "| policy_loss        | -0.002405172 |\n",
      "| serial_timesteps   | 13312        |\n",
      "| time_elapsed       | 1.95e+03     |\n",
      "| total_timesteps    | 106496       |\n",
      "| value_loss         | 2.2577927    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=114688, episode_reward=1487.93 +/- 668.05\n",
      "Episode length: 8020.20 +/- 3587.53\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0016605406  |\n",
      "| clipfrac           | 0.011173503   |\n",
      "| ep_len_mean        | 693           |\n",
      "| ep_reward_mean     | 122           |\n",
      "| explained_variance | -0.974        |\n",
      "| fps                | 13            |\n",
      "| n_updates          | 14            |\n",
      "| policy_entropy     | 0.36663014    |\n",
      "| policy_loss        | -0.0021488606 |\n",
      "| serial_timesteps   | 14336         |\n",
      "| time_elapsed       | 2.65e+03      |\n",
      "| total_timesteps    | 114688        |\n",
      "| value_loss         | 2.2809167     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=122880, episode_reward=1360.10 +/- 747.38\n",
      "Episode length: 7307.90 +/- 4007.98\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0031993962  |\n",
      "| clipfrac           | 0.03663737    |\n",
      "| ep_len_mean        | 745           |\n",
      "| ep_reward_mean     | 131           |\n",
      "| explained_variance | -0.449        |\n",
      "| fps                | 12            |\n",
      "| n_updates          | 15            |\n",
      "| policy_entropy     | 0.38266933    |\n",
      "| policy_loss        | -0.0026595944 |\n",
      "| serial_timesteps   | 15360         |\n",
      "| time_elapsed       | 3.26e+03      |\n",
      "| total_timesteps    | 122880        |\n",
      "| value_loss         | 2.1078672     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=131072, episode_reward=1007.28 +/- 760.58\n",
      "Episode length: 5425.60 +/- 4080.22\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0020061578  |\n",
      "| clipfrac           | 0.017293295   |\n",
      "| ep_len_mean        | 846           |\n",
      "| ep_reward_mean     | 150           |\n",
      "| explained_variance | -0.382        |\n",
      "| fps                | 19            |\n",
      "| n_updates          | 16            |\n",
      "| policy_entropy     | 0.3688779     |\n",
      "| policy_loss        | -0.0023585367 |\n",
      "| serial_timesteps   | 16384         |\n",
      "| time_elapsed       | 3.9e+03       |\n",
      "| total_timesteps    | 131072        |\n",
      "| value_loss         | 2.0092082     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.ppo2.ppo2.PPO2 at 0x20f024fd488>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.learn(total_timesteps=65536, callback=eval_callback)\n",
    "model.learn(total_timesteps=131072, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando o Modelo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1602\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "obs = EvalEnv.reset()\n",
    "dones = 0\n",
    "for t in itertools.count():\n",
    "    EvalEnv.render()\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = EvalEnv.step(action)\n",
    "    if dones:\n",
    "        break\n",
    "    \n",
    "print(t)\n",
    "# print(EvalEnv.n_pipes)\n",
    "EvalEnv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvando o Modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trained_models/NormPPO2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gravando um Episódio: (Not Working)\n",
    "\n",
    "Criando o ambiente gravado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "video_folder = 'videos/'\n",
    "video_length = 100\n",
    "\n",
    "VideoEnv = DummyVecEnv([lambda: FlappyBirdEnv()])\n",
    "\n",
    "obs = VideoEnv.reset()\n",
    "\n",
    "VideoEnv = VecVideoRecorder(VideoEnv, video_folder,\n",
    "                       record_video_trigger=lambda x: x == 0, video_length=video_length,\n",
    "                       name_prefix=\"FlappyBird\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rodando o episódio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Env returned None on render(). Disabling further rendering for video recorder by marking as disabled: path=D:\\Codigos\\RL\\FlappyBirdPPO2\\videos\\FlappyBird-step-101-to-step-201.mp4 metadata_path=D:\\Codigos\\RL\\FlappyBirdPPO2\\videos\\FlappyBird-step-101-to-step-201.meta.json\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to  D:\\Codigos\\RL\\FlappyBirdPPO2\\videos\\FlappyBird-step-101-to-step-201.mp4\n"
     ]
    }
   ],
   "source": [
    "obs = VideoEnv.reset()\n",
    "\n",
    "for _ in range(video_length + 1):\n",
    "    VideoEnv.render()\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _, _ = VideoEnv.step(action)\n",
    "\n",
    "VideoEnv.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flappy Bird com Visão Computacional\n",
    "\n",
    "A implementar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FlappyBird-v0\"\n",
    "\n",
    "import gym\n",
    "\n",
    "from stable_baselines.common.policies import CnnLstmPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "env_vision = gym.make(ENV_NAME)\n",
    "# Optional: PPO2 requires a vectorized environment to run\n",
    "# the env is now wrapped automatically when passing it to the constructor\n",
    "env_vision = DummyVecEnv([lambda: env_vision])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelVision = PPO2(CnnLstmPolicy, env_vision, n_steps = 512, nminibatches = 1, lam = 0.98, gamma = 0.999, noptepochs= 15, ent_coef= 0.01, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelVision.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env_vision.reset()\n",
    "dones = 0\n",
    "while not dones:\n",
    "    action, _states = modelVision.predict(obs)\n",
    "    obs, rewards, dones, info = env_vision.step(action)\n",
    "    env.render()\n",
    "\n",
    "env_vision.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
